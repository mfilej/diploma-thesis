Automatically generated by Mendeley Desktop 1.16.1
Any changes to this file will be lost if it is regenerated by Mendeley.

BibTeX export options can be customized via Preferences -> BibTeX in Mendeley Desktop

@misc{Kanungo1999,
author = {Kanungo, Tapas},
publisher = {Cambridge University Press},
title = {{UMDHMM: Hidden Markov Model Toolkit}},
url = {http://www.kanungo.com/software/software.html},
year = {1999}
}
@article{Szymanski,
author = {Szymanski, Grzegorz},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Szymanski - Unknown - Hidden Markov Models Suitable for Text Generation(2).pdf:pdf},
keywords = {GRZEGORZ SZYMANSKI,ZYGMUNT CIOTA},
pages = {2--5},
title = {{Hidden Markov Models Suitable for Text Generation}},
volume = {1}
}
@phdthesis{Zhao2011,
author = {Zhao, James},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - ZhaoJames.pdf:pdf},
school = {The University of Melbourne},
title = {{Hidden Markov Models with Multiple Observation Processes}},
year = {2007}
}
@article{Guedon2003,
abstract = {This article addresses the estimation of hidden semi-Markov chains from nonstationary discrete sequences. Hidden semi-Markov chains are particularly useful to model the succes- sion of homogeneous zones or segments along sequences. A discrete hidden semi-Markov chain is composed of a nonobservable state process, which is a semi-Markov chain, and a discrete output process. Hidden semi-Markov chains generalize hidden Markov chains and enable the modeling of various durational structures. From an algorithmic point of view, a new forward-backward algorithm is proposed whose complexity is similar to that of the Viterbi algorithm in terms of sequence length (quadratic in the worst case in time and linear in space). This opens the way to the maximum likelihood estimation of hidden semi-Markov chains from long sequences. This statistical modeling approach is illustrated by the analysis of branching and flowering patterns in plants. Key},
author = {Gu{\'{e}}don, Yann},
doi = {10.1198/1061860032030},
file = {:Users/miha/Downloads/JCGSguedon2003.pdf:pdf},
issn = {1061-8600},
journal = {Journal of Computational and Graphical Statistics},
keywords = {Censoring,EM algorithm,Forward-backward algorithm,Nonparametric max- imum likelihood,Plant structure analysis,Smoothing algorithm,Viterbi algorithm. 1.},
number = {3},
pages = {604--639},
title = {{Estimating Hidden Semi-Markov Chains From Discrete Sequences}},
volume = {12},
year = {2003}
}
@misc{Zhou2005,
abstract = {We have developed the following web servers for protein structural modeling and analysis at : THUMBUP, UMDHMM(TMHP) and TUPS, predictors of transmembrane helical protein topology based on a mean-burial-propensity scale of amino acid residues (THUMBUP), hidden Markov model (UMDHMM(TMHP)) and their combinations (TUPS); SPARKS 2.0 and SP(3), two profile–profile alignment methods, that match input query sequence(s) to structural templates by integrating sequence profile with knowledge-based structural score (SPARKS 2.0) and structure-derived profile (SP(3)); DFIRE, a knowledge-based potential for scoring free energy of monomers (DMONOMER), loop conformations (DLOOP), mutant stability (DMUTANT) and binding affinity of protein–protein/peptide/DNA complexes (DCOMPLEX {\&} DDNA); TCD, a program for protein-folding rate and transition-state analysis of small globular proteins; and DOGMA, a web-server that allows comparative analysis of domain combinations between plant and other 55 organisms. These servers provide tools for prediction and/or analysis of proteins on the secondary structure, tertiary structure and interaction levels, respectively. },
author = {Zhou, Hongyi and Zhang, Chi and Liu, Song and Zhou, Yaoqi},
booktitle = {Nucleic Acids Research},
doi = {10.1093/nar/gki360},
issn = {0305-1048 (Print)},
language = {eng},
month = {jul},
number = {Web Server issue},
pages = {W193--7},
pmid = {15980453},
title = {{Web-based toolkits for topology prediction of transmembrane helical proteins, fold recognition, structure and binding scoring, folding-kinetics analysis and comparative analysis of domain combinations}},
volume = {33},
year = {2005}
}
@article{Li2000,
abstract = {Hidden Markov models (HMM) are stochastic models capable of statistical learning and classification. They have been applied in speech recognition and handwriting recognition because of their great adaptability and versatility in handling sequential signals. On the other hand, as these models have a complex structure and also because the involved data sets usually contain uncertainty, it is difficult to analyze the multiple observation training problem without certain assumptions. For many years researchers have used the training equations of Levinson (1983) in speech and handwriting applications, simply assuming that all observations are independent of each other. This paper presents a formal treatment of HMM multiple observation training without imposing the above assumption. In this treatment, the multiple observation probability is expressed as a combination of individual observation probabilities without losing generality. This combinatorial method gives one more freedom in making different dependence-independence assumptions. By generalizing Baum's auxiliary function into this framework and building up an associated objective function using the Lagrange multiplier method, it is proven that the derived training equations guarantee the maximization of the objective function. Furthermore, we show that Levinson's training equations can be easily derived as a special case in this treatment},
author = {Li, Xiaolin Li Xiaolin and Parizeau, M. and Plamondon, R.},
doi = {10.1109/34.845379},
file = {:Users/miha/Downloads/P971225.pdf:pdf},
isbn = {doi:10.1109/34.845379},
issn = {0162-8828},
journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
number = {4},
pages = {371--377},
title = {{Training hidden Markov models with multiple observations-a combinatorial method}},
volume = {22},
year = {2000}
}
@article{Hudak1989,
author = {Hudak, P.},
file = {:Users/miha/Downloads/p359-hudak.pdf:pdf},
journal = {ACM Computing Surveys},
keywords = {and phrases,data abstraction,referential transparency,types},
number = {3},
pages = {359--411},
title = {{Conception, evolution, and application of functional programming language}},
volume = {21},
year = {1989}
}
@inproceedings{Thung2013,
author = {Thung, F and Bissyand{\'{e}}, T F and Lo, D and Jiang, L},
booktitle = {Software Maintenance and Reengineering (CSMR), 2013 17th European Conference on},
doi = {10.1109/CSMR.2013.41},
issn = {1534-5351},
keywords = {Collaboration,Encoding,Facebook,GitHub,Measurement,PageRank,Social coding,Software,Web pages,building tool support,collaborative software development,developer-developer network,developer-developer relationship graph,development paradigm,graph theory,groupware,network structure,project hosting platform,project-project network,project-project relationship graph,social coding site,social networking (online),social programmer,software engineering,umbrella site},
month = {mar},
pages = {323--326},
title = {{Network Structure of Social Coding in GitHub}},
year = {2013}
}
@book{Beck2003,
author = {Beck, Kent},
publisher = {Addison-Wesley Professional},
title = {{Test-driven development: by example}},
year = {2003}
}
@misc{elixir/exunit,
booktitle = {elixir-lang.org},
title = {{ExUnit - ExUnit v1.2.5}},
urldate = {2016-05-25}
}
@article{OConnell2011,
author = {O'Connell, Jared and H{\o}jsgaard, S{\o}ren},
journal = {Journal of Statistical Software},
number = {4},
pages = {1--22},
title = {{Hidden Semi Markov Models for Multiple Observation Sequences: The mhsmm Package for R}},
url = {http://www.jstatsoft.org/v39/i04/},
volume = {39},
year = {2011}
}
@article{Mann2006,
abstract = {Application of Hidden Markov Models to long observation sequences entails the computation of extremely small probabilities. These probabilities introduce numerical instability in the computations used to determine the probability of an observed se- quence given a model, the most likely sequence of states, and the maximum likelihood model updates given an observation sequence. This paper explains how to handle small probabilities by working with the logarithms of probabilities, rather than resorting to alternative rescaling procedures.},
author = {Mann, Tobias P},
file = {:Users/miha/Downloads/hmm{\_}scaling{\_}revised.pdf:pdf},
journal = {An HMM scaling tutorial},
pages = {1--8},
title = {{Numerically Stable Hidden Markov Model Implementation}},
year = {2006}
}
@inproceedings{Papadakis2011,
author = {Papadakis, Manolis and Sagonas, Konstantinos},
booktitle = {Proceedings of the 10th ACM SIGPLAN workshop on Erlang},
file = {:Users/miha/Downloads/proper{\_}types.pdf:pdf},
organization = {ACM},
pages = {39--50},
title = {{A PropEr integration of types and function specifications with property-based testing}},
year = {2011}
}
@misc{fmap/markov,
booktitle = {github.com},
title = {markov},
url = {https://github.com/fmap/markov},
urldate = {2016-05-30}
}
@misc{Rabiner1989,
abstract = {This tutorial provides an overview of the basic theory of hidden Markov models (HMMs) as originated by L.E. Baum and T. Petrie (1966) and gives practical details on methods of implementation of the theory along with a description of selected applications of the theory to distinct problems in speech recognition. Results from a number of original sources are combined to provide a single source of acquiring the background required to pursue further this area of research. The author first reviews the theory of discrete Markov chains and shows how the concept of hidden states, where the observation is a probabilistic function of the state, can be used effectively. The theory is illustrated with two simple examples, namely coin-tossing, and the classic balls-in-urns system. Three fundamental problems of HMMs are noted and several practical techniques for solving these problems are given. The various types of HMMs that have been studied, including ergodic as well as left-right models, are described},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Rabiner, Lawrence R.},
booktitle = {Proceedings of the IEEE},
doi = {10.1109/5.18626},
eprint = {arXiv:1011.1669v3},
file = {:Users/miha/Dropbox/Docs/thesis/Sources/rabiner-tutorial-on-hmm-ocr.pdf:pdf},
isbn = {1558601244},
issn = {15582256},
number = {2},
pages = {257--286},
pmid = {21920608},
title = {{A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition}},
volume = {77},
year = {1989}
}
@article{Ghazal2013,
abstract = {There is a tremendous interest in big data by academia, industry and a large user base. Several commercial and open source providers unleashed a variety of products to support big data storage and processing. As these products mature, there is a need to evaluate and compare the performance of these systems. In this paper, we present BigBench, an end-to-end big data benchmark proposal. The underlying business model of BigBench is a product retailer. The proposal covers a data model and synthetic data generator that addresses the variety, velocity and volume aspects of big data systems containing structured, semi-structured and unstructured data. The structured part of the BigBench data model is adopted from the TPC-DS benchmark, which is enriched with semi-structured and unstructured data components. The semi-structured part captures registered and guest user clicks on the retailer's website. The unstructured data captures product reviews submitted online. The data generator designed for BigBench provides scalable volumes of raw data based on a scale factor. The BigBench workload is designed around a set of queries against the data model. From a business prospective, the queries cover the different categories of big data analytics proposed by McKinsey. From a technical prospective, the queries are designed to span three different dimensions based on data sources, query processing types and analytic techniques. We illustrate the feasibility of BigBench by implementing it on the Teradata Aster Database. The test includes generating and loading a 200 Gigabyte BigBench data set and testing the workload by executing the BigBench queries (written using Teradata Aster SQL-MR) and reporting their response times.},
archivePrefix = {arXiv},
arxivId = {arXiv:1401.1406v2},
author = {Ghazal, Ahmad and Rabl, Tilmann and Hu, Minqing and Raab, Francois and Poess, Meikel and Crolette, Alain and Jacobsen, Hans-Arno},
doi = {10.1145/2463676.2463712},
eprint = {arXiv:1401.1406v2},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Ghazal et al. - 2013 - Bigbench Towards an industry standard benchmark for big data analytics(2).pdf:pdf},
isbn = {9781450320375},
issn = {07308078},
journal = {Proceedings of the 2013 ACM SIGMOD International Conference on Management of Data},
keywords = {benchmarking,big data,map reduce},
pages = {1197--1208},
pmid = {83466038},
title = {{Bigbench: Towards an industry standard benchmark for big data analytics}},
year = {2013}
}
@article{Hughes1989,
author = {Hughes, John},
file = {:Users/miha/Downloads/The Computer Journal-1989-Hughes-98-107.pdf:pdf},
journal = {The computer journal},
number = {2},
pages = {98--107},
publisher = {Br Computer Soc},
title = {{Why functional programming matters}},
volume = {32},
year = {1989}
}
@inproceedings{Ramesh1992,
author = {Ramesh, P and Wilpon, J G},
booktitle = {Acoustics, Speech, and Signal Processing, 1992. ICASSP-92., 1992 IEEE International Conference on},
doi = {10.1109/ICASSP.1992.225892},
issn = {1520-6149},
keywords = {Automatic speech recognition,Computational complexity,Databases,Decoding,Equations,Error analysis,HMM algorithms,Hidden Markov models,Speech recognition,Viterbi algorithm,Viterbi decoding,automatic speech recognition,computational complexity,connected speech recognition,databases,hidden Markov models,inhomogeneous HMM,postprocessing state duration penalty,recognition error rates,speech recognition,state durations,temporal structure,time-dependent state transitions},
month = {mar},
pages = {381--384 vol.1},
title = {{Modeling state durations in hidden Markov models for automatic speech recognition}},
volume = {1},
year = {1992}
}
@article{Walt2011,
author = {{Van Der Walt}, Stefan and Colbert, S Chris and Varoquaux, Gael},
journal = {Computing in Science {\&} Engineering},
number = {2},
pages = {22--30},
publisher = {AIP Publishing},
title = {{The NumPy array: a structure for efficient numerical computation}},
volume = {13},
year = {2011}
}
@misc{guyz/HMM,
author = {Zyskind, Guy},
booktitle = {github.com},
title = {{HMM}},
url = {https://github.com/guyz/HMM},
urldate = {2016-05-25}
}
@book{Thomas2014,
author = {Thomas, Dave},
publisher = {Pragmatic Bookshelf},
title = {{Programming Elixir}},
year = {2014}
}
@inproceedings{Gael2008,
author = {{Van Gael}, Jurgen and Saatci, Yunus and Teh, Yee Whye and Ghahramani, Zoubin},
booktitle = {Proceedings of the 25th international conference on Machine learning},
organization = {ACM},
pages = {1088--1095},
title = {{Beam sampling for the infinite hidden Markov model}},
year = {2008}
}
@article{Comino2007,
author = {Comino, Stefano and Manenti, Fabio M and Parisi, Maria Laura},
journal = {Research Policy},
number = {10},
pages = {1575--1586},
publisher = {Elsevier},
title = {{From planning to mature: On the success of open source projects}},
volume = {36},
year = {2007}
}
@article{Lustrek2004,
author = {Lu{\v{s}}trek, Mitja},
file = {:Users/miha/Downloads/Luscenje{\_}podatkov{\_}s{\_}skritimi{\_}markovskimi{\_}modeli-IS-04.pdf:pdf},
institution = {Institut Jozef Stefan},
title = {{Lu{\v{s}}{\v{c}}enje podatkov s skritimi markovskimi modeli}},
year = {2004}
}
@book{Bishop2006,
author = {Bishop, Christopher},
isbn = {978-0-387-31073-2},
publisher = {Springer},
title = {{Pattern recognition and machine learning}},
year = {2006}
}
@article{Yun2013,
author = {Yun, Jonghyun},
title = {{A MATLAB toolbox to identify RNA-protein binding sites in HITS-CLIP}},
year = {2013}
}
@incollection{Li2007,
author = {Li, Huiqing and Thompson, Simon},
booktitle = {Implementation and Application of Functional Languages},
file = {:Users/miha/Downloads/TestingErlangRefactoringsWithQuickCheck.pdf:pdf},
pages = {19--36},
publisher = {Springer},
title = {{Testing Erlang refactorings with quickcheck}},
year = {2007}
}
@misc{jmschrei/pomegranate,
booktitle = {github.com},
title = {pomegranate},
url = {https://github.com/jmschrei/pomegranate},
urldate = {2016-05-30}
}
@book{Juric2015,
author = {Juri{\'{c}}, Sa{\v{s}}a},
publisher = {Manning Publ.},
title = {{Elixir in action}},
year = {2015}
}
@misc{GHAHRAMANI2001,
abstract = {We provide a tutorial on learning and inference in hidden Markov models in the context of the recent literature on Bayesian networks. This perspective makes it possible to consider novel generalizations of hidden Markov models with multiple hidden state variables, multiscale representations, and mixed discrete and continuous variables. Although exact inference in these generalizations is usually intractable, one can use approximate inference algorithms such as Markov chain sampling and variational methods. We describe how such methods are applied to these generalized hidden Markov models. We conclude this review with a discussion of Bayesian methods for model selection in generalized HMMs.},
author = {GHAHRAMANI, ZOUBIN},
booktitle = {International Journal of Pattern Recognition and Artificial Intelligence},
doi = {10.1142/S0218001401000836},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/GHAHRAMANI - 2001 - AN INTRODUCTION TO HIDDEN MARKOV MODELS AND BAYESIAN NETWORKS.pdf:pdf},
isbn = {981-02-4564-5},
issn = {0218-0014},
number = {01},
pages = {9--42},
pmid = {18428778},
title = {{AN INTRODUCTION TO HIDDEN MARKOV MODELS AND BAYESIAN NETWORKS}},
volume = {15},
year = {2001}
}
@inproceedings{Dufaux2000,
author = {Dufaux, Alain and Besacier, Laurent and Ansorge, Michael and Pellandini, Fausto},
booktitle = {Signal Processing Conference, 2000 10th European},
organization = {IEEE},
pages = {1--4},
title = {{Automatic sound detection and recognition for noisy environment}},
year = {2000}
}
@inproceedings{Sagonas2005,
author = {Sagonas, Konstantinos},
booktitle = {Proceedings of the ACM SIGPLAN Workshop on the Evaluation of Software Defect Detection Tools},
file = {:Users/miha/Downloads/10.1.1.66.699.pdf:pdf},
organization = {Citeseer},
title = {{Experience from developing the Dialyzer: A static analysis tool detecting defects in Erlang applications}},
year = {2005}
}
@misc{r/hmm,
booktitle = {cran.r-project.org},
title = {{R-HMM}},
url = {https://cran.r-project.org/web/packages/HMM/HMM.pdf},
urldate = {2016-05-30}
}
@article{Edwards2004,
abstract = {We describe a method that can make a scanned, handwritten$\backslash$nmediaeval latin manuscript accessible to full text search. A$\backslash$ngeneralized HMM is Þtted, using transcribed latin to obtain$\backslash$na transition model and one example each of 22 letters to$\backslash$nobtain an emission model. We show results for unigram,$\backslash$nbigram and trigram models. Our method transcribes 25 pages$\backslash$nof a manuscript of Terence with fair accuracy (75 {\%} of$\backslash$nletters correctly transcribed). Search results are very$\backslash$nstrong; we use examples of variant spellings to demonstrate$\backslash$nthat the search respects the ink of the document.$\backslash$nFurthermore, our model produces fair searches on a document$\backslash$nfrom which we obtained no training data. 1. Intoduction$\backslash$nThere are many large corpora of handwritten scanned$\backslash$ndocuments, and their number is growing rapidly. Collections$\backslash$nrange from the complete works of Mark Twain to thousands of$\backslash$npages of zoological notes spanning two centuries. Large$\backslash$nscale analyses of such corpora},
author = {Edwards, Jaety and Teh, Yee W. and Bock, Roger and Maire, Michael and Vesom, Grace and Forsyth, David A.},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Edwards et al. - 2004 - Making latin manuscripts searchable using gHMM's.pdf:pdf},
journal = {Advances in Neural Information Processing Systems},
pages = {385--392},
title = {{Making latin manuscripts searchable using gHMM's}},
volume = {17},
year = {2004}
}
@book{Pavesic2010,
address = {Ljubljana},
author = {Pave{\v{s}}i{\'{c}}, Nikola},
edition = {2},
isbn = {978-961-243-145-7},
publisher = {Zalo{\v{z}}ba FE in FRI},
title = {{Informacija in kodi}},
year = {2010}
}
@article{Maqsud2015,
author = {Maqsud, Umar},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Maqsud - 2015 - Synthetic Text Generation for Sentiment Analysis(2).pdf:pdf},
number = {Wassa},
pages = {156--161},
title = {{Synthetic Text Generation for Sentiment Analysis}},
year = {2015}
}
@inproceedings{Dabbish2012,
address = {New York, NY, USA},
author = {Dabbish, Laura and Stuart, Colleen and Tsay, Jason and Herbsleb, Jim},
booktitle = {Proceedings of the ACM 2012 Conference on Computer Supported Cooperative Work},
doi = {10.1145/2145204.2145396},
isbn = {978-1-4503-1086-4},
keywords = {awareness,collaboration,coordination,open source software development,social computing,transparency},
pages = {1277--1286},
publisher = {ACM},
series = {CSCW '12},
title = {{Social Coding in GitHub: Transparency and Collaboration in an Open Software Repository}},
url = {http://doi.acm.org/10.1145/2145204.2145396},
year = {2012}
}
@techreport{Bilmes1997,
abstract = {We describe the maximum-likelihood parameter estimation problem and howthe Expectation- Maximization (EM) algorithm can be used for its solution. We first describe the abstract form of the EM algorithm as it is often given in the literature. We then develop the EM pa- rameter estimation procedure for two applications: 1) finding the parameters of a mixture of Gaussian densities, and 2) finding the parameters of a hidden Markov model (HMM) (i.e., the Baum-Welch algorithm) for both discrete and Gaussian mixture observation models. We derive the update equations in fairly explicit detail but we do not prove any conver- gence properties. We try to emphasize intuition rather than mathematical rigor.},
author = {Bilmes, Jeff},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Bilmes - 1997 - A Gentle Tutorial of the {\{}EM{\}} algorithm and its application to Parameter Estimation for {\{}G{\}}aussian Mixture and Hidden {\{}M.pdf:pdf}},
institution = {ICSI},
number = {TR-97-021},
title = {{A Gentle Tutorial of the EM algorithm and its application to Parameter Estimation for Gaussian Mixture and Hidden Markov Models}},
year = {1997}
}
@article{Sonnenburg2007,
author = {Sonnenburg, Soren and Braun, Mikio L and Ong, Cheng Soon and Bengio, Samy and Bottou, Leon and Holmes, Geoffrey and LeCun, Yann and Mueller, Klaus-Robert and Pereira, Fernando and Rasmussen, Carl E and Raetsch, Gunnar and Schoelkopf, Bernhard and Smola, Alexander and Vincent, Pascal and Weston, Jason and Williamson, Robert},
file = {:Users/miha/Downloads/Sonnenburg{\_}Need2007.pdf:pdf},
journal = {Journal of Machine Learning Research},
pages = {2443--2466},
title = {{The need for open source software in machine learning}},
volume = {8},
year = {2007}
}
@misc{Ramage2007,
author = {Ramage, Daniel},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Unknown - Unknown - cs229-hmm.pdf:pdf},
title = {{Hidden Markov Models Fundamentals}},
year = {2007}
}
@misc{sf/ghmm,
booktitle = {sourceforge.net},
title = {{GHMM: General Hidden Markov Model library}},
url = {http://ghmm.sourceforge.net},
urldate = {2016-06-30}
}
@article{Schliep2004,
author = {Schliep, Alexander and Georgi, Benjamin and Rungsarityotin, Wasinee and Costa, I and Schonhuth, A},
journal = {Proceedings of the Heinz-billing-price},
pages = {121--135},
title = {{The general hidden markov model library: Analyzing systems with unobservable states}},
volume = {2004},
year = {2004}
}
@misc{parroty/excheck,
booktitle = {github.com},
title = {{ExCheck}},
url = {https://github.com/parroty/excheck},
urldate = {2016-05-25}
}
@article{Goldwater2009,
abstract = {Since the experiments of Saffran et al. [Saffran, J., Aslin, R., {\&} Newport, E. (1996). Statistical learning in 8-month-old infants. Science, 274, 1926-1928], there has been a great deal of interest in the question of how statistical regularities in the speech stream might be used by infants to begin to identify individual words. In this work, we use computational modeling to explore the effects of different assumptions the learner might make regarding the nature of words - in particular, how these assumptions affect the kinds of words that are segmented from a corpus of transcribed child-directed speech. We develop several models within a Bayesian ideal observer framework, and use them to examine the consequences of assuming either that words are independent units, or units that help to predict other units. We show through empirical and theoretical results that the assumption of independence causes the learner to undersegment the corpus, with many two- and three-word sequences (e.g. what's that, do you, in the house) misidentified as individual words. In contrast, when the learner assumes that words are predictive, the resulting segmentation is far more accurate. These results indicate that taking context into account is important for a statistical word segmentation strategy to be successful, and raise the possibility that even young infants may be able to exploit more subtle statistical patterns than have usually been considered. ?? 2009 Elsevier B.V. All rights reserved.},
author = {Goldwater, Sharon and Griffiths, Thomas L. and Johnson, Mark},
doi = {10.1016/j.cognition.2009.03.008},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Goldwater, Griffiths, Johnson - 2009 - A Bayesian framework for word segmentation Exploring the effects of context(2).pdf:pdf},
issn = {00100277},
journal = {Cognition},
keywords = {Bayesian,Computational modeling,Language acquisition,Word segmentation},
number = {1},
pages = {21--54},
pmid = {19409539},
publisher = {Elsevier B.V.},
title = {{A Bayesian framework for word segmentation: Exploring the effects of context}},
volume = {112},
year = {2009}
}
@inproceedings{Carlsson2006,
address = {New York, NY, USA},
author = {Carlsson, Richard and R{\'{e}}mond, Micka{\"{e}}l},
booktitle = {Proceedings of the 2006 ACM SIGPLAN Workshop on Erlang},
doi = {10.1145/1159789.1159791},
isbn = {1-59593-490-1},
keywords = {Erlang,agile methods,frameworks,unit testing},
pages = {1},
publisher = {ACM},
series = {ERLANG '06},
title = {{EUnit: A Lightweight Unit Testing Framework for Erlang}},
url = {http://doi.acm.org/10.1145/1159789.1159791},
year = {2006}
}
@article{Xu1996,
author = {Xu, Lei and Jordan, Michael I},
journal = {Neural computation},
number = {1},
pages = {129--151},
publisher = {MIT Press},
title = {{On convergence properties of the EM algorithm for Gaussian mixtures}},
volume = {8},
year = {1996}
}
@book{Armstrong2007,
author = {Armstrong, Joe},
publisher = {Pragmatic Bookshelf},
title = {{Programming Erlang: software for a concurrent world}},
year = {2007}
}
@inproceedings{McDonald2013,
address = {New York, NY, USA},
author = {McDonald, Nora and Goggins, Sean},
booktitle = {CHI '13 Extended Abstracts on Human Factors in Computing Systems},
doi = {10.1145/2468356.2468382},
isbn = {978-1-4503-1952-2},
keywords = {open source software,performance,social computing},
pages = {139--144},
publisher = {ACM},
series = {CHI EA '13},
title = {{Performance and Participation in Open Source Software on GitHub}},
url = {http://doi.acm.org/10.1145/2468356.2468382},
year = {2013}
}
@misc{elixir/map,
booktitle = {elixir-lang.org},
title = {{Map - Elixir v1.2.5}},
url = {http://elixir-lang.org/docs/stable/elixir/Map.html},
urldate = {2016-05-24}
}
@article{Johnson2009,
abstract = {One of the reasons nonparametric Bayesian inference is attracting attention in computational linguistics is because it provides a principled way of learning the units of generalization together with their probabilities. Adaptor grammars are a framework for defining a variety of hierarchical nonparametric Bayesian models. This paper investigates some of the choices that arise in formulating adaptor grammars and associated inference procedures, and shows that they can have a dramatic impact on performance in an unsupervised word segmentation task. With appropriate adaptor grammars and inference procedures we achieve an 87{\%} word token f-score on the standard Brent version of the Bernstein-Ratner corpus, which is an error reduction of over 35{\%} over the best previously reported results for this corpus.},
author = {Johnson, Mark and Goldwater, Sharon},
doi = {10.3115/1620754.1620800},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Johnson, Goldwater - 2009 - Improving nonparameteric Bayesian inference experiments on unsupervised word segmentation with adaptor gramm.pdf:pdf},
isbn = {9781932432411},
issn = {1932432418},
journal = {Proceedings of Human Language Technologies The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics},
number = {June},
pages = {317--325},
title = {{Improving nonparameteric Bayesian inference: experiments on unsupervised word segmentation with adaptor grammars}},
year = {2009}
}
@misc{web/nolicense,
title = {{No License - Choose a License}},
url = {http://choosealicense.com/no-license/},
urldate = {2016-06-02}
}
@misc{hmmlearn/hmmlearn,
booktitle = {github.com},
title = {hmmlearn},
url = {https://github.com/hmmlearn/hmmlearn},
urldate = {2016-05-30}
}
@article{Ming2014a,
abstract = {Data generation is a key issue in big data benchmarking that aims to generate application-specific data sets to meet the 4V requirements of big data. Specifically, big data generators need to generate scalable data (Volume) of different types (Variety) under controllable generation rates (Velocity) while keeping the important characteristics of raw data (Veracity). This gives rise to various new challenges about how we design generators efficiently and successfully. To date, most existing techniques can only generate limited types of data and support specific big data systems such as Hadoop. Hence we develop a tool, called Big Data Generator Suite (BDGS), to efficiently generate scalable big data while employing data models derived from real data to preserve data veracity. The effectiveness of BDGS is demonstrated by developing six data generators covering three representative data types (structured, semi-structured and unstructured) and three data sources (text, graph, and table data).},
archivePrefix = {arXiv},
arxivId = {1401.5465},
author = {Ming, Zijian and Luo, Chunjie and Gao, Wanling and Han, Rui and Yang, Qiang and Wang, Lei and Zhan, Jianfeng},
doi = {10.1007/978-3-319-10596-3_11},
eprint = {1401.5465},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Ming et al. - 2014 - BDGS A scalable big data generator suite in big data benchmarking.pdf:pdf},
isbn = {9783319105956},
issn = {16113349},
journal = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
keywords = {Benchmark,Big data,Data generator,Scalable,Veracity},
pages = {138--154},
title = {{BDGS: A scalable big data generator suite in big data benchmarking}},
volume = {8585},
year = {2014}
}
@misc{rabinererratum,
author = {Rahimi, Ali},
title = {{An Erratum for ``A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition''}},
url = {http://alumni.media.mit.edu/{~}rahimi/rabiner/rabiner-errata/rabiner-errata.html},
urldate = {2016-05-27}
}
@article{Cappe2001,
author = {Capp{\'{e}}, Olivier},
journal = {URL: http://www. tsi. enst. fr/{\~{}} cappe/h2m},
title = {{H2M: A set of MATLAB/OCTAVE functions for the EM estimation of mixtures and hidden Markov models}},
year = {2001}
}
@book{Gyergyk1988,
address = {Ljubljana},
author = {Gyergy{\'{e}}k, Ludvik},
language = {Slovenian},
publisher = {Fakulteta za elektrotehniko},
title = {{Teorija o informacijah}},
year = {1988}
}
@article{Determann2006,
author = {Determann, Lothar},
issn = {10863818},
journal = {Berkeley Technology Law Journal},
number = {4},
pages = {1421--1498},
publisher = {Temporary Publisher},
title = {{Dangerous Liaisons—Software Combinations as Derivative Works? Distribution, Installation, and Execution of Linked Programs Under Copyright Law, Commercial Licenses, and the GPL}},
url = {http://www.jstor.org/stable/24118692},
volume = {21},
year = {2006}
}
@book{Bruen2005,
address = {Hoboken, N.J},
author = {Bruen, Aiden},
isbn = {978-0-471-65317-2},
publisher = {Wiley-Interscience},
title = {{Cryptography, information theory, and error-correction : a handbook for the 21st century}},
year = {2005}
}
@inproceedings{Sagonas2008,
author = {Sagonas, Konstantinos and Luna, Daniel},
booktitle = {Proceedings of the 7th ACM SIGPLAN Workshop on Erlang},
file = {:Users/miha/Downloads/wrangler.pdf:pdf},
organization = {ACM},
pages = {73--82},
title = {{Gradual typing of erlang programs: a wrangler experience}},
year = {2008}
}
@misc{jeremyjh/dialyxir,
author = {Huffman, Jeremy},
booktitle = {github.com},
title = {{Dialyxir}},
url = {https://github.com/jeremyjh/dialyxir},
urldate = {2016-05-24}
}
@inproceedings{Rigoll1996,
author = {Rigoll, Gerhard and Kosmala, Andreas and Rattland, J and Neukirchen, Ch},
booktitle = {Pattern Recognition, 1996., Proceedings of the 13th International Conference on},
organization = {IEEE},
pages = {205--209},
title = {{A comparison between continuous and discrete density hidden Markov models for cursive handwriting recognition}},
volume = {2},
year = {1996}
}
@article{Cohen2010,
author = {Cohen, Shay B. and Blei, David M. and Smith, Noah A.},
file = {:Users/miha/Library/Application Support/Mendeley Desktop/Downloaded/Cohen, Blei, Smith - 2010 - Variational Inference for Adaptor Grammars.pdf:pdf},
isbn = {1-932432-65-5},
issn = {1932432655},
journal = {Naacl-2010},
number = {June},
pages = {564--572},
title = {{Variational Inference for Adaptor Grammars}},
year = {2010}
}
@article{Stewart2006,
abstract = {What differentiates successful from unsuccessful open source software projects? This paper develops and tests a model of the impacts of license restrictiveness and organizational sponsorship on two indicators of success: user interest in, and development activity on, open source software development projects. Using data gathered from Freshmeat.net and project home pages, the main conclusions derived from the analysis are that (1) license restrictiveness and organizational sponsorship interact to influence user perceptions of the likely utility of open source software in such a way that users are most attracted to projects that are sponsored by nonmarket organizations and that employ nonrestrictive licenses, and (2) licensing and sponsorship address complementary developer motivations such that the influence of licensing on development activity depends on what kind of organizational sponsor a project has. Theoretical and practical implications are discussed, and the paper outlines several avenues for future research.},
author = {Stewart, Katherine J. and Ammeter, Anthony P. and Maruping, Likoebe M.},
doi = {10.1287/isre.1060.0082},
journal = {Information Systems Research},
number = {2},
pages = {126--144},
title = {{Impacts of License Choice and Organizational Sponsorship on User Interest and Development Activity in Open Source Software Projects}},
url = {http://pubsonline.informs.org/doi/abs/10.1287/isre.1060.0082},
volume = {17},
year = {2006}
}
@article{Pedregosa2011,
author = {Pedregosa, Fabian and Varoquaux, Ga{\"{e}}l and Gramfort, Alexandre and Michel, Vincent and Thirion, Bertrand and Grisel, Olivier and Blondel, Mathieu and Prettenhofer, Peter and Weiss, Ron and Dubourg, Vincent and Vanderplas, Jake and Passos, Alexandre and Cournapeau, David and Brucher, Matthieu and Perrot, Matthieu and Duchesnay, {\'{E}}douard},
issn = {1532-4435},
journal = {J. Mach. Learn. Res.},
month = {nov},
pages = {2825--2830},
publisher = {JMLR.org},
title = {{Scikit-learn: Machine Learning in Python}},
url = {http://dl.acm.org/citation.cfm?id=1953048.2078195},
volume = {12},
year = {2011}
}
