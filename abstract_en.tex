Natural language generation is the task of producing text that feels natural
to the reader. The goal of this diploma thesis is to establish the degree to
which natural language generation can be achieved using hidden markov models.
The diploma thesis covers probability and information theories that allow the
definition of hidden markov models and describes how such models can be used
for the purpose of text generation. Available tools for working with hidden
markov models are reviewed and assesed for their suitability for generating
text. A library for hidden markov models is implemented in Elixir. Two of the
reviewed tools and the implemented library are used to generate text from a
corpus of written slovenian language. A metric for comparing generated texts
is chosen and used to compare the models as well as comparing the generated
texts to the corpus.
